{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe99081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (22.0.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (0.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: gym in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from gym) (1.22.3)\n",
      "Requirement already satisfied: keras in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: keras-rl2 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from keras-rl2) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.24.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (58.0.4)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (13.0.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.12)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.22.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.34.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.7.4.3)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (2.8.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (3.19.4)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (2.8.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip3 install tensorflow\n",
    "#!pip3 install tensorflow==2.4.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b8a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from flask) (2.11.3)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from flask) (8.0.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from flask) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from click>=5.1->flask) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dgarr\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->flask) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b0e88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RL libraries\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "#Neural network libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Math libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "#Web server libraries\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "\n",
    "#Visual libraries\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99c1f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [09/May/2022 22:45:05] \"GET / HTTP/1.1\" 200 -\n",
      "[2022-05-09 22:45:05,718] ERROR in app: Exception on /plot.png [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\dgarr\\AppData\\Local\\Temp/ipykernel_15852/1191939518.py\", line 24, in testEnv\n",
      "    power, angles, n_state, reward, done, info = env.step(action)\n",
      "  File \"C:\\Users\\dgarr\\AppData\\Local\\Temp/ipykernel_15852/1844835785.py\", line 52, in step\n",
      "    self.error = abs(float(self.powerRef) - self.genPowerEuler)\n",
      "ValueError: could not convert string to float: 'plot.png'\n",
      "127.0.0.1 - - [09/May/2022 22:45:05] \"GET /plot.png HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/May/2022 22:45:08] \"POST / HTTP/1.1\" 302 -\n",
      "127.0.0.1 - - [09/May/2022 22:45:08] \"GET /50 HTTP/1.1\" 302 -\n",
      "[2022-05-09 22:45:08,997] ERROR in app: Exception on /[8.785146800263615, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 9.085146800263614, 9.085146800263614, 9.185146800263613, 9.285146800263613, 9.185146800263613, 9.085146800263614, 8.985146800263614, 8.885146800263614, 8.985146800263614, 9.085146800263614, 9.085146800263614, 8.985146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.785146800263615, 8.685146800263615, 8.585146800263615, 8.585146800263615, 8.585146800263615, 8.485146800263616, 8.585146800263615, 8.685146800263615, 8.785146800263615, 8.885146800263614, 8.985146800263614, 8.885146800263614, 8.785146800263615, 8.885146800263614, 8.985146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.785146800263615, 8.885146800263614, 8.885146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 9.085146800263614, 11.487489264394695, 11.387489264394695, 11.387489264394695, 11.387489264394695, 11.287489264394695, 11.287489264394695, 11.287489264394695, 11.187489264394696, 11.087489264394696, 11.087489264394696, 10.987489264394696, 10.987489264394696, 10.887489264394697, 10.787489264394697, 10.687489264394697, 10.687489264394697, 10.587489264394698, 10.587489264394698, 10.687489264394697, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.487489264394698, 10.587489264394698, 10.687489264394697, 10.787489264394697, 10.787489264394697, 10.787489264394697, 10.687489264394697, 12.017128357727046, 12.117128357727045, 12.117128357727045, 12.217128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.217128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.317128357727045, 12.217128357727045, 12.217128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.517128357727044, 12.617128357727044, 12.517128357727044, 12.617128357727044, 12.717128357727043, 12.617128357727044, 12.617128357727044, 12.617128357727044, 12.717128357727043, 12.617128357727044, 12.517128357727044, 12.417128357727044, 12.317128357727045, 12.217128357727045, 12.117128357727045, 12.217128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.317128357727045, 12.317128357727045, 12.217128357727045, 12.217128357727045, 12.217128357727045, 12.117128357727045, 12.217128357727045, 12.117128357727045, 12.117128357727045, 12.217128357727045, 12.117128357727045, 12.017128357727046, 12.117128357727045, 12.017128357727046, 12.117128357727045, 12.217128357727045, 12.217128357727045, 12.117128357727045, 12.017128357727046, 12.117128357727045, 7.292906302760493, 7.192906302760493, 7.092906302760493, 7.192906302760493, 7.292906302760493, 7.292906302760493, 7.392906302760492, 7.292906302760493, 7.392906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.5929063027604915, 7.692906302760491, 7.792906302760491, 7.792906302760491, 7.692906302760491, 7.692906302760491, 7.792906302760491, 7.792906302760491, 7.692906302760491, 7.5929063027604915, 7.5929063027604915, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.392906302760492, 7.392906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.392906302760492, 7.492906302760492, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.392906302760492, 7.292906302760493, 7.192906302760493, 7.292906302760493, 7.292906302760493, 7.192906302760493, 7.292906302760493, 7.392906302760492, 7.492906302760492, 7.492906302760492, 7.492906302760492, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.620669911399933, 9.720669911399932, 9.620669911399933, 9.620669911399933, 9.520669911399933, 9.420669911399933, 9.420669911399933, 9.420669911399933, 9.320669911399934, 9.420669911399933, 9.520669911399933, 9.520669911399933, 9.520669911399933, 9.620669911399933, 9.620669911399933, 9.520669911399933, 9.520669911399933, 9.520669911399933, 9.420669911399933, 9.420669911399933, 9.520669911399933, 9.620669911399933, 9.620669911399933, 9.720669911399932, 9.820669911399932, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.920669911399932, 10.020669911399931, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.820669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.820669911399932, 9.920669911399932, 10.020669911399931, 10.120669911399931, 10.120669911399931, 10.020669911399931, 10.120669911399931, 10.020669911399931, 10.020669911399931, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.620669911399933, 13.890319651045857, 13.890319651045857, 13.990319651045857, 14.090319651045856, 13.990319651045857, 13.890319651045857, 13.790319651045857, 13.890319651045857, 13.990319651045857, 13.990319651045857, 13.990319651045857, 14.090319651045856, 14.190319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 14.390319651045855, 14.490319651045855, 14.390319651045855, 14.290319651045856, 14.190319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.290319651045856, 14.390319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.490319651045855, 14.490319651045855, 14.490319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.490319651045855, 14.590319651045855, 14.590319651045855, 14.590319651045855, 14.490319651045855, 14.590319651045855, 14.490319651045855, 14.490319651045855, 14.590319651045855, 14.590319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 10.823276449575243, 10.923276449575242, 10.923276449575242, 11.023276449575242, 11.023276449575242, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.723276449575243, 10.823276449575243, 10.923276449575242, 10.923276449575242, 11.023276449575242, 11.023276449575242, 11.023276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.023276449575242, 11.123276449575242, 11.223276449575241, 11.123276449575242, 11.023276449575242, 11.123276449575242, 11.223276449575241, 11.123276449575242, 11.123276449575242, 11.023276449575242, 10.923276449575242, 10.923276449575242, 11.023276449575242, 10.923276449575242, 10.923276449575242, 10.923276449575242, 10.823276449575243, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.623276449575243, 10.523276449575244, 10.423276449575244, 10.323276449575244, 10.223276449575245, 10.323276449575244, 10.223276449575245, 10.223276449575245, 10.223276449575245, 10.123276449575245, 10.223276449575245, 8.6847407238031, 8.6847407238031, 8.7847407238031, 8.8847407238031, 8.9847407238031, 8.9847407238031, 8.8847407238031, 8.7847407238031, 8.7847407238031, 8.6847407238031, 8.584740723803101, 8.584740723803101, 8.484740723803101, 8.484740723803101, 8.584740723803101, 8.584740723803101, 8.584740723803101, 8.584740723803101, 8.6847407238031, 8.584740723803101, 8.6847407238031, 8.7847407238031, 8.8847407238031, 8.8847407238031, 8.8847407238031, 8.8847407238031, 8.9847407238031, 9.0847407238031, 9.0847407238031, 9.184740723803099, 9.184740723803099, 9.0847407238031, 9.184740723803099, 9.284740723803099, 9.184740723803099, 9.284740723803099, 9.384740723803098, 9.484740723803098, 9.384740723803098, 9.384740723803098, 9.284740723803099, 9.384740723803098, 9.384740723803098, 9.484740723803098, 9.484740723803098, 9.584740723803097, 9.484740723803098, 9.484740723803098, 9.584740723803097, 9.484740723803098, 9.484740723803098, 9.384740723803098, 9.384740723803098, 9.484740723803098, 9.384740723803098, 9.484740723803098, 9.584740723803097, 9.684740723803097, 9.784740723803097, 9.784740723803097, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.179990113213142, 11.279990113213142, 11.179990113213142, 11.279990113213142, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.479990113213141, 11.57999011321314, 11.57999011321314, 11.67999011321314, 11.57999011321314, 11.67999011321314, 11.57999011321314, 11.57999011321314, 11.479990113213141, 11.379990113213141, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.379990113213141, 11.479990113213141, 11.379990113213141, 11.379990113213141, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.179990113213142, 11.079990113213142, 11.079990113213142, 11.079990113213142, 11.179990113213142, 11.079990113213142, 10.979990113213143, 10.879990113213143, 10.879990113213143, 10.979990113213143, 10.979990113213143, 10.879990113213143, 10.879990113213143, 10.779990113213143, 10.879990113213143, 10.879990113213143, 10.779990113213143, 10.679990113213144, 10.679990113213144, 10.679990113213144, 10.679990113213144, 10.579990113213144, 10.679990113213144, 10.779990113213143, 10.679990113213144, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.23006051123538, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.330060511235379, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.23006051123538, 12.13006051123538, 12.13006051123538, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.330060511235379, 12.430060511235379, 12.330060511235379, 12.23006051123538, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.430060511235379, 12.530060511235378, 12.430060511235379, 12.330060511235379, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.530060511235378, 12.630060511235378, 12.730060511235378, 12.830060511235377, 12.730060511235378, 12.630060511235378, 12.630060511235378, 12.530060511235378, 12.530060511235378, 12.430060511235379, 12.330060511235379] [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\dgarr\\AppData\\Local\\Temp/ipykernel_15852/1191939518.py\", line 24, in testEnv\n",
      "    power, angles, n_state, reward, done, info = env.step(action)\n",
      "  File \"C:\\Users\\dgarr\\AppData\\Local\\Temp/ipykernel_15852/1844835785.py\", line 52, in step\n",
      "    self.error = abs(float(self.powerRef) - self.genPowerEuler)\n",
      "ValueError: could not convert string to float: '[8.785146800263615, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 9.085146800263614, 9.085146800263614, 9.185146800263613, 9.285146800263613, 9.185146800263613, 9.085146800263614, 8.985146800263614, 8.885146800263614, 8.985146800263614, 9.085146800263614, 9.085146800263614, 8.985146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.785146800263615, 8.685146800263615, 8.585146800263615, 8.585146800263615, 8.585146800263615, 8.485146800263616, 8.585146800263615, 8.685146800263615, 8.785146800263615, 8.885146800263614, 8.985146800263614, 8.885146800263614, 8.785146800263615, 8.885146800263614, 8.985146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.785146800263615, 8.885146800263614, 8.885146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 9.085146800263614, 11.487489264394695, 11.387489264394695, 11.387489264394695, 11.387489264394695, 11.287489264394695, 11.287489264394695, 11.287489264394695, 11.187489264394696, 11.087489264394696, 11.087489264394696, 10.987489264394696, 10.987489264394696, 10.887489264394697, 10.787489264394697, 10.687489264394697, 10.687489264394697, 10.587489264394698, 10.587489264394698, 10.687489264394697, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.487489264394698, 10.587489264394698, 10.687489264394697, 10.787489264394697, 10.787489264394697, 10.787489264394697, 10.687489264394697, 12.017128357727046, 12.117128357727045, 12.117128357727045, 12.217128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.217128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.317128357727045, 12.217128357727045, 12.217128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.517128357727044, 12.617128357727044, 12.517128357727044, 12.617128357727044, 12.717128357727043, 12.617128357727044, 12.617128357727044, 12.617128357727044, 12.717128357727043, 12.617128357727044, 12.517128357727044, 12.417128357727044, 12.317128357727045, 12.217128357727045, 12.117128357727045, 12.217128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.317128357727045, 12.317128357727045, 12.217128357727045, 12.217128357727045, 12.217128357727045, 12.117128357727045, 12.217128357727045, 12.117128357727045, 12.117128357727045, 12.217128357727045, 12.117128357727045, 12.017128357727046, 12.117128357727045, 12.017128357727046, 12.117128357727045, 12.217128357727045, 12.217128357727045, 12.117128357727045, 12.017128357727046, 12.117128357727045, 7.292906302760493, 7.192906302760493, 7.092906302760493, 7.192906302760493, 7.292906302760493, 7.292906302760493, 7.392906302760492, 7.292906302760493, 7.392906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.5929063027604915, 7.692906302760491, 7.792906302760491, 7.792906302760491, 7.692906302760491, 7.692906302760491, 7.792906302760491, 7.792906302760491, 7.692906302760491, 7.5929063027604915, 7.5929063027604915, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.392906302760492, 7.392906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.392906302760492, 7.492906302760492, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.392906302760492, 7.292906302760493, 7.192906302760493, 7.292906302760493, 7.292906302760493, 7.192906302760493, 7.292906302760493, 7.392906302760492, 7.492906302760492, 7.492906302760492, 7.492906302760492, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.620669911399933, 9.720669911399932, 9.620669911399933, 9.620669911399933, 9.520669911399933, 9.420669911399933, 9.420669911399933, 9.420669911399933, 9.320669911399934, 9.420669911399933, 9.520669911399933, 9.520669911399933, 9.520669911399933, 9.620669911399933, 9.620669911399933, 9.520669911399933, 9.520669911399933, 9.520669911399933, 9.420669911399933, 9.420669911399933, 9.520669911399933, 9.620669911399933, 9.620669911399933, 9.720669911399932, 9.820669911399932, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.920669911399932, 10.020669911399931, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.820669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.820669911399932, 9.920669911399932, 10.020669911399931, 10.120669911399931, 10.120669911399931, 10.020669911399931, 10.120669911399931, 10.020669911399931, 10.020669911399931, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.620669911399933, 13.890319651045857, 13.890319651045857, 13.990319651045857, 14.090319651045856, 13.990319651045857, 13.890319651045857, 13.790319651045857, 13.890319651045857, 13.990319651045857, 13.990319651045857, 13.990319651045857, 14.090319651045856, 14.190319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 14.390319651045855, 14.490319651045855, 14.390319651045855, 14.290319651045856, 14.190319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.290319651045856, 14.390319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.490319651045855, 14.490319651045855, 14.490319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.490319651045855, 14.590319651045855, 14.590319651045855, 14.590319651045855, 14.490319651045855, 14.590319651045855, 14.490319651045855, 14.490319651045855, 14.590319651045855, 14.590319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 10.823276449575243, 10.923276449575242, 10.923276449575242, 11.023276449575242, 11.023276449575242, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.723276449575243, 10.823276449575243, 10.923276449575242, 10.923276449575242, 11.023276449575242, 11.023276449575242, 11.023276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.023276449575242, 11.123276449575242, 11.223276449575241, 11.123276449575242, 11.023276449575242, 11.123276449575242, 11.223276449575241, 11.123276449575242, 11.123276449575242, 11.023276449575242, 10.923276449575242, 10.923276449575242, 11.023276449575242, 10.923276449575242, 10.923276449575242, 10.923276449575242, 10.823276449575243, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.623276449575243, 10.523276449575244, 10.423276449575244, 10.323276449575244, 10.223276449575245, 10.323276449575244, 10.223276449575245, 10.223276449575245, 10.223276449575245, 10.123276449575245, 10.223276449575245, 8.6847407238031, 8.6847407238031, 8.7847407238031, 8.8847407238031, 8.9847407238031, 8.9847407238031, 8.8847407238031, 8.7847407238031, 8.7847407238031, 8.6847407238031, 8.584740723803101, 8.584740723803101, 8.484740723803101, 8.484740723803101, 8.584740723803101, 8.584740723803101, 8.584740723803101, 8.584740723803101, 8.6847407238031, 8.584740723803101, 8.6847407238031, 8.7847407238031, 8.8847407238031, 8.8847407238031, 8.8847407238031, 8.8847407238031, 8.9847407238031, 9.0847407238031, 9.0847407238031, 9.184740723803099, 9.184740723803099, 9.0847407238031, 9.184740723803099, 9.284740723803099, 9.184740723803099, 9.284740723803099, 9.384740723803098, 9.484740723803098, 9.384740723803098, 9.384740723803098, 9.284740723803099, 9.384740723803098, 9.384740723803098, 9.484740723803098, 9.484740723803098, 9.584740723803097, 9.484740723803098, 9.484740723803098, 9.584740723803097, 9.484740723803098, 9.484740723803098, 9.384740723803098, 9.384740723803098, 9.484740723803098, 9.384740723803098, 9.484740723803098, 9.584740723803097, 9.684740723803097, 9.784740723803097, 9.784740723803097, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.179990113213142, 11.279990113213142, 11.179990113213142, 11.279990113213142, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.479990113213141, 11.57999011321314, 11.57999011321314, 11.67999011321314, 11.57999011321314, 11.67999011321314, 11.57999011321314, 11.57999011321314, 11.479990113213141, 11.379990113213141, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.379990113213141, 11.479990113213141, 11.379990113213141, 11.379990113213141, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.179990113213142, 11.079990113213142, 11.079990113213142, 11.079990113213142, 11.179990113213142, 11.079990113213142, 10.979990113213143, 10.879990113213143, 10.879990113213143, 10.979990113213143, 10.979990113213143, 10.879990113213143, 10.879990113213143, 10.779990113213143, 10.879990113213143, 10.879990113213143, 10.779990113213143, 10.679990113213144, 10.679990113213144, 10.679990113213144, 10.679990113213144, 10.579990113213144, 10.679990113213144, 10.779990113213143, 10.679990113213144, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.23006051123538, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.330060511235379, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.23006051123538, 12.13006051123538, 12.13006051123538, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.330060511235379, 12.430060511235379, 12.330060511235379, 12.23006051123538, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.430060511235379, 12.530060511235378, 12.430060511235379, 12.330060511235379, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.530060511235378, 12.630060511235378, 12.730060511235378, 12.830060511235377, 12.730060511235378, 12.630060511235378, 12.630060511235378, 12.530060511235378, 12.530060511235378, 12.430060511235379, 12.330060511235379]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/May/2022 22:45:08] \"GET /%5B8.785146800263615%2C%208.885146800263614%2C%208.885146800263614%2C%208.885146800263614%2C%208.985146800263614%2C%208.985146800263614%2C%209.085146800263614%2C%208.985146800263614%2C%209.085146800263614%2C%209.085146800263614%2C%209.185146800263613%2C%209.285146800263613%2C%209.185146800263613%2C%209.085146800263614%2C%208.985146800263614%2C%208.885146800263614%2C%208.985146800263614%2C%209.085146800263614%2C%209.085146800263614%2C%208.985146800263614%2C%208.985146800263614%2C%209.085146800263614%2C%208.985146800263614%2C%208.985146800263614%2C%208.885146800263614%2C%208.785146800263615%2C%208.685146800263615%2C%208.585146800263615%2C%208.585146800263615%2C%208.585146800263615%2C%208.485146800263616%2C%208.585146800263615%2C%208.685146800263615%2C%208.785146800263615%2C%208.885146800263614%2C%208.985146800263614%2C%208.885146800263614%2C%208.785146800263615%2C%208.885146800263614%2C%208.985146800263614%2C%208.885146800263614%2C%208.985146800263614%2C%208.985146800263614%2C%208.985146800263614%2C%208.985146800263614%2C%208.885146800263614%2C%208.885146800263614%2C%208.985146800263614%2C%208.985146800263614%2C%208.885146800263614%2C%208.885146800263614%2C%208.885146800263614%2C%208.885146800263614%2C%208.785146800263615%2C%208.885146800263614%2C%208.885146800263614%2C%208.985146800263614%2C%209.085146800263614%2C%208.985146800263614%2C%209.085146800263614%2C%2011.487489264394695%2C%2011.387489264394695%2C%2011.387489264394695%2C%2011.387489264394695%2C%2011.287489264394695%2C%2011.287489264394695%2C%2011.287489264394695%2C%2011.187489264394696%2C%2011.087489264394696%2C%2011.087489264394696%2C%2010.987489264394696%2C%2010.987489264394696%2C%2010.887489264394697%2C%2010.787489264394697%2C%2010.687489264394697%2C%2010.687489264394697%2C%2010.587489264394698%2C%2010.587489264394698%2C%2010.687489264394697%2C%2010.587489264394698%2C%2010.487489264394698%2C%2010.487489264394698%2C%2010.387489264394699%2C%2010.487489264394698%2C%2010.587489264394698%2C%2010.487489264394698%2C%2010.387489264394699%2C%2010.487489264394698%2C%2010.587489264394698%2C%2010.587489264394698%2C%2010.487489264394698%2C%2010.487489264394698%2C%2010.387489264394699%2C%2010.487489264394698%2C%2010.587489264394698%2C%2010.487489264394698%2C%2010.487489264394698%2C%2010.587489264394698%2C%2010.587489264394698%2C%2010.587489264394698%2C%2010.587489264394698%2C%2010.587489264394698%2C%2010.587489264394698%2C%2010.487489264394698%2C%2010.387489264394699%2C%2010.487489264394698%2C%2010.387489264394699%2C%2010.487489264394698%2C%2010.587489264394698%2C%2010.487489264394698%2C%2010.487489264394698%2C%2010.387489264394699%2C%2010.487489264394698%2C%2010.487489264394698%2C%2010.587489264394698%2C%2010.687489264394697%2C%2010.787489264394697%2C%2010.787489264394697%2C%2010.787489264394697%2C%2010.687489264394697%2C%2012.017128357727046%2C%2012.117128357727045%2C%2012.117128357727045%2C%2012.217128357727045%2C%2012.317128357727045%2C%2012.317128357727045%2C%2012.317128357727045%2C%2012.317128357727045%2C%2012.317128357727045%2C%2012.217128357727045%2C%2012.317128357727045%2C%2012.417128357727044%2C%2012.417128357727044%2C%2012.317128357727045%2C%2012.217128357727045%2C%2012.217128357727045%2C%2012.317128357727045%2C%2012.317128357727045%2C%2012.317128357727045%2C%2012.417128357727044%2C%2012.417128357727044%2C%2012.517128357727044%2C%2012.617128357727044%2C%2012.517128357727044%2C%2012.617128357727044%2C%2012.717128357727043%2C%2012.617128357727044%2C%2012.617128357727044%2C%2012.617128357727044%2C%2012.717128357727043%2C%2012.617128357727044%2C%2012.517128357727044%2C%2012.417128357727044%2C%2012.317128357727045%2C%2012.217128357727045%2C%2012.117128357727045%2C%2012.217128357727045%2C%2012.317128357727045%2C%2012.417128357727044%2C%2012.417128357727044%2C%2012.317128357727045%2C%2012.317128357727045%2C%2012.217128357727045%2C%2012.217128357727045%2C%2012.217128357727045%2C%2012.117128357727045%2C%2012.217128357727045%2C%2012.117128357727045%2C%2012.117128357727045%2C%2012.217128357727045%2C%2012.117128357727045%2C%2012.017128357727046%2C%2012.117128357727045%2C%2012.017128357727046%2C%2012.117128357727045%2C%2012.217128357727045%2C%2012.217128357727045%2C%2012.117128357727045%2C%2012.017128357727046%2C%2012.117128357727045%2C%207.292906302760493%2C%207.192906302760493%2C%207.092906302760493%2C%207.192906302760493%2C%207.292906302760493%2C%207.292906302760493%2C%207.392906302760492%2C%207.292906302760493%2C%207.392906302760492%2C%207.492906302760492%2C%207.5929063027604915%2C%207.492906302760492%2C%207.492906302760492%2C%207.5929063027604915%2C%207.5929063027604915%2C%207.492906302760492%2C%207.492906302760492%2C%207.492906302760492%2C%207.5929063027604915%2C%207.5929063027604915%2C%207.692906302760491%2C%207.792906302760491%2C%207.792906302760491%2C%207.692906302760491%2C%207.692906302760491%2C%207.792906302760491%2C%207.792906302760491%2C%207.692906302760491%2C%207.5929063027604915%2C%207.5929063027604915%2C%207.5929063027604915%2C%207.492906302760492%2C%207.492906302760492%2C%207.392906302760492%2C%207.392906302760492%2C%207.492906302760492%2C%207.5929063027604915%2C%207.492906302760492%2C%207.5929063027604915%2C%207.492906302760492%2C%207.5929063027604915%2C%207.492906302760492%2C%207.392906302760492%2C%207.492906302760492%2C%207.492906302760492%2C%207.492906302760492%2C%207.5929063027604915%2C%207.492906302760492%2C%207.492906302760492%2C%207.392906302760492%2C%207.292906302760493%2C%207.192906302760493%2C%207.292906302760493%2C%207.292906302760493%2C%207.192906302760493%2C%207.292906302760493%2C%207.392906302760492%2C%207.492906302760492%2C%207.492906302760492%2C%207.492906302760492%2C%209.920669911399932%2C%209.920669911399932%2C%209.820669911399932%2C%209.720669911399932%2C%209.820669911399932%2C%209.920669911399932%2C%209.920669911399932%2C%209.820669911399932%2C%209.720669911399932%2C%209.620669911399933%2C%209.720669911399932%2C%209.620669911399933%2C%209.620669911399933%2C%209.520669911399933%2C%209.420669911399933%2C%209.420669911399933%2C%209.420669911399933%2C%209.320669911399934%2C%209.420669911399933%2C%209.520669911399933%2C%209.520669911399933%2C%209.520669911399933%2C%209.620669911399933%2C%209.620669911399933%2C%209.520669911399933%2C%209.520669911399933%2C%209.520669911399933%2C%209.420669911399933%2C%209.420669911399933%2C%209.520669911399933%2C%209.620669911399933%2C%209.620669911399933%2C%209.720669911399932%2C%209.820669911399932%2C%209.920669911399932%2C%209.920669911399932%2C%209.820669911399932%2C%209.920669911399932%2C%2010.020669911399931%2C%209.920669911399932%2C%209.820669911399932%2C%209.720669911399932%2C%209.820669911399932%2C%209.820669911399932%2C%209.820669911399932%2C%209.720669911399932%2C%209.820669911399932%2C%209.820669911399932%2C%209.920669911399932%2C%2010.020669911399931%2C%2010.120669911399931%2C%2010.120669911399931%2C%2010.020669911399931%2C%2010.120669911399931%2C%2010.020669911399931%2C%2010.020669911399931%2C%209.920669911399932%2C%209.820669911399932%2C%209.720669911399932%2C%209.620669911399933%2C%2013.890319651045857%2C%2013.890319651045857%2C%2013.990319651045857%2C%2014.090319651045856%2C%2013.990319651045857%2C%2013.890319651045857%2C%2013.790319651045857%2C%2013.890319651045857%2C%2013.990319651045857%2C%2013.990319651045857%2C%2013.990319651045857%2C%2014.090319651045856%2C%2014.190319651045856%2C%2014.290319651045856%2C%2014.290319651045856%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.290319651045856%2C%2014.390319651045855%2C%2014.490319651045855%2C%2014.390319651045855%2C%2014.290319651045856%2C%2014.190319651045856%2C%2014.290319651045856%2C%2014.290319651045856%2C%2014.390319651045855%2C%2014.290319651045856%2C%2014.390319651045855%2C%2014.490319651045855%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.490319651045855%2C%2014.490319651045855%2C%2014.490319651045855%2C%2014.490319651045855%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.290319651045856%2C%2014.290319651045856%2C%2014.290319651045856%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.490319651045855%2C%2014.590319651045855%2C%2014.590319651045855%2C%2014.590319651045855%2C%2014.490319651045855%2C%2014.590319651045855%2C%2014.490319651045855%2C%2014.490319651045855%2C%2014.590319651045855%2C%2014.590319651045855%2C%2014.490319651045855%2C%2014.390319651045855%2C%2014.390319651045855%2C%2014.290319651045856%2C%2010.823276449575243%2C%2010.923276449575242%2C%2010.923276449575242%2C%2011.023276449575242%2C%2011.023276449575242%2C%2010.923276449575242%2C%2010.823276449575243%2C%2010.723276449575243%2C%2010.723276449575243%2C%2010.823276449575243%2C%2010.923276449575242%2C%2010.923276449575242%2C%2011.023276449575242%2C%2011.023276449575242%2C%2011.023276449575242%2C%2011.123276449575242%2C%2011.123276449575242%2C%2011.123276449575242%2C%2011.123276449575242%2C%2011.123276449575242%2C%2011.123276449575242%2C%2011.023276449575242%2C%2011.123276449575242%2C%2011.223276449575241%2C%2011.123276449575242%2C%2011.023276449575242%2C%2011.123276449575242%2C%2011.223276449575241%2C%2011.123276449575242%2C%2011.123276449575242%2C%2011.023276449575242%2C%2010.923276449575242%2C%2010.923276449575242%2C%2011.023276449575242%2C%2010.923276449575242%2C%2010.923276449575242%2C%2010.923276449575242%2C%2010.823276449575243%2C%2010.923276449575242%2C%2010.823276449575243%2C%2010.723276449575243%2C%2010.823276449575243%2C%2010.823276449575243%2C%2010.823276449575243%2C%2010.823276449575243%2C%2010.823276449575243%2C%2010.923276449575242%2C%2010.823276449575243%2C%2010.723276449575243%2C%2010.623276449575243%2C%2010.523276449575244%2C%2010.423276449575244%2C%2010.323276449575244%2C%2010.223276449575245%2C%2010.323276449575244%2C%2010.223276449575245%2C%2010.223276449575245%2C%2010.223276449575245%2C%2010.123276449575245%2C%2010.223276449575245%2C%208.6847407238031%2C%208.6847407238031%2C%208.7847407238031%2C%208.8847407238031%2C%208.9847407238031%2C%208.9847407238031%2C%208.8847407238031%2C%208.7847407238031%2C%208.7847407238031%2C%208.6847407238031%2C%208.584740723803101%2C%208.584740723803101%2C%208.484740723803101%2C%208.484740723803101%2C%208.584740723803101%2C%208.584740723803101%2C%208.584740723803101%2C%208.584740723803101%2C%208.6847407238031%2C%208.584740723803101%2C%208.6847407238031%2C%208.7847407238031%2C%208.8847407238031%2C%208.8847407238031%2C%208.8847407238031%2C%208.8847407238031%2C%208.9847407238031%2C%209.0847407238031%2C%209.0847407238031%2C%209.184740723803099%2C%209.184740723803099%2C%209.0847407238031%2C%209.184740723803099%2C%209.284740723803099%2C%209.184740723803099%2C%209.284740723803099%2C%209.384740723803098%2C%209.484740723803098%2C%209.384740723803098%2C%209.384740723803098%2C%209.284740723803099%2C%209.384740723803098%2C%209.384740723803098%2C%209.484740723803098%2C%209.484740723803098%2C%209.584740723803097%2C%209.484740723803098%2C%209.484740723803098%2C%209.584740723803097%2C%209.484740723803098%2C%209.484740723803098%2C%209.384740723803098%2C%209.384740723803098%2C%209.484740723803098%2C%209.384740723803098%2C%209.484740723803098%2C%209.584740723803097%2C%209.684740723803097%2C%209.784740723803097%2C%209.784740723803097%2C%2011.279990113213142%2C%2011.179990113213142%2C%2011.179990113213142%2C%2011.279990113213142%2C%2011.179990113213142%2C%2011.179990113213142%2C%2011.179990113213142%2C%2011.279990113213142%2C%2011.179990113213142%2C%2011.279990113213142%2C%2011.379990113213141%2C%2011.279990113213142%2C%2011.379990113213141%2C%2011.479990113213141%2C%2011.57999011321314%2C%2011.57999011321314%2C%2011.67999011321314%2C%2011.57999011321314%2C%2011.67999011321314%2C%2011.57999011321314%2C%2011.57999011321314%2C%2011.479990113213141%2C%2011.379990113213141%2C%2011.379990113213141%2C%2011.279990113213142%2C%2011.379990113213141%2C%2011.279990113213142%2C%2011.379990113213141%2C%2011.379990113213141%2C%2011.479990113213141%2C%2011.379990113213141%2C%2011.379990113213141%2C%2011.279990113213142%2C%2011.179990113213142%2C%2011.179990113213142%2C%2011.179990113213142%2C%2011.079990113213142%2C%2011.079990113213142%2C%2011.079990113213142%2C%2011.179990113213142%2C%2011.079990113213142%2C%2010.979990113213143%2C%2010.879990113213143%2C%2010.879990113213143%2C%2010.979990113213143%2C%2010.979990113213143%2C%2010.879990113213143%2C%2010.879990113213143%2C%2010.779990113213143%2C%2010.879990113213143%2C%2010.879990113213143%2C%2010.779990113213143%2C%2010.679990113213144%2C%2010.679990113213144%2C%2010.679990113213144%2C%2010.679990113213144%2C%2010.579990113213144%2C%2010.679990113213144%2C%2010.779990113213143%2C%2010.679990113213144%2C%2012.23006051123538%2C%2012.13006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.330060511235379%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.13006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.13006051123538%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.330060511235379%2C%2012.330060511235379%2C%2012.330060511235379%2C%2012.23006051123538%2C%2012.330060511235379%2C%2012.330060511235379%2C%2012.23006051123538%2C%2012.13006051123538%2C%2012.13006051123538%2C%2012.23006051123538%2C%2012.330060511235379%2C%2012.330060511235379%2C%2012.330060511235379%2C%2012.430060511235379%2C%2012.430060511235379%2C%2012.330060511235379%2C%2012.430060511235379%2C%2012.330060511235379%2C%2012.23006051123538%2C%2012.330060511235379%2C%2012.430060511235379%2C%2012.430060511235379%2C%2012.430060511235379%2C%2012.530060511235378%2C%2012.430060511235379%2C%2012.330060511235379%2C%2012.23006051123538%2C%2012.23006051123538%2C%2012.330060511235379%2C%2012.430060511235379%2C%2012.430060511235379%2C%2012.530060511235378%2C%2012.630060511235378%2C%2012.730060511235378%2C%2012.830060511235377%2C%2012.730060511235378%2C%2012.630060511235378%2C%2012.630060511235378%2C%2012.530060511235378%2C%2012.530060511235378%2C%2012.430060511235379%2C%2012.330060511235379%5D HTTP/1.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[8.785146800263615, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 9.085146800263614, 9.085146800263614, 9.185146800263613, 9.285146800263613, 9.185146800263613, 9.085146800263614, 8.985146800263614, 8.885146800263614, 8.985146800263614, 9.085146800263614, 9.085146800263614, 8.985146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.785146800263615, 8.685146800263615, 8.585146800263615, 8.585146800263615, 8.585146800263615, 8.485146800263616, 8.585146800263615, 8.685146800263615, 8.785146800263615, 8.885146800263614, 8.985146800263614, 8.885146800263614, 8.785146800263615, 8.885146800263614, 8.985146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.885146800263614, 8.985146800263614, 8.985146800263614, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.885146800263614, 8.785146800263615, 8.885146800263614, 8.885146800263614, 8.985146800263614, 9.085146800263614, 8.985146800263614, 9.085146800263614, 11.487489264394695, 11.387489264394695, 11.387489264394695, 11.387489264394695, 11.287489264394695, 11.287489264394695, 11.287489264394695, 11.187489264394696, 11.087489264394696, 11.087489264394696, 10.987489264394696, 10.987489264394696, 10.887489264394697, 10.787489264394697, 10.687489264394697, 10.687489264394697, 10.587489264394698, 10.587489264394698, 10.687489264394697, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.587489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.587489264394698, 10.487489264394698, 10.487489264394698, 10.387489264394699, 10.487489264394698, 10.487489264394698, 10.587489264394698, 10.687489264394697, 10.787489264394697, 10.787489264394697, 10.787489264394697, 10.687489264394697, 12.017128357727046, 12.117128357727045, 12.117128357727045, 12.217128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.217128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.317128357727045, 12.217128357727045, 12.217128357727045, 12.317128357727045, 12.317128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.517128357727044, 12.617128357727044, 12.517128357727044, 12.617128357727044, 12.717128357727043, 12.617128357727044, 12.617128357727044, 12.617128357727044, 12.717128357727043, 12.617128357727044, 12.517128357727044, 12.417128357727044, 12.317128357727045, 12.217128357727045, 12.117128357727045, 12.217128357727045, 12.317128357727045, 12.417128357727044, 12.417128357727044, 12.317128357727045, 12.317128357727045, 12.217128357727045, 12.217128357727045, 12.217128357727045, 12.117128357727045, 12.217128357727045, 12.117128357727045, 12.117128357727045, 12.217128357727045, 12.117128357727045, 12.017128357727046, 12.117128357727045, 12.017128357727046, 12.117128357727045, 12.217128357727045, 12.217128357727045, 12.117128357727045, 12.017128357727046, 12.117128357727045, 7.292906302760493, 7.192906302760493, 7.092906302760493, 7.192906302760493, 7.292906302760493, 7.292906302760493, 7.392906302760492, 7.292906302760493, 7.392906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.5929063027604915, 7.692906302760491, 7.792906302760491, 7.792906302760491, 7.692906302760491, 7.692906302760491, 7.792906302760491, 7.792906302760491, 7.692906302760491, 7.5929063027604915, 7.5929063027604915, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.392906302760492, 7.392906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.392906302760492, 7.492906302760492, 7.492906302760492, 7.492906302760492, 7.5929063027604915, 7.492906302760492, 7.492906302760492, 7.392906302760492, 7.292906302760493, 7.192906302760493, 7.292906302760493, 7.292906302760493, 7.192906302760493, 7.292906302760493, 7.392906302760492, 7.492906302760492, 7.492906302760492, 7.492906302760492, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.620669911399933, 9.720669911399932, 9.620669911399933, 9.620669911399933, 9.520669911399933, 9.420669911399933, 9.420669911399933, 9.420669911399933, 9.320669911399934, 9.420669911399933, 9.520669911399933, 9.520669911399933, 9.520669911399933, 9.620669911399933, 9.620669911399933, 9.520669911399933, 9.520669911399933, 9.520669911399933, 9.420669911399933, 9.420669911399933, 9.520669911399933, 9.620669911399933, 9.620669911399933, 9.720669911399932, 9.820669911399932, 9.920669911399932, 9.920669911399932, 9.820669911399932, 9.920669911399932, 10.020669911399931, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.820669911399932, 9.820669911399932, 9.720669911399932, 9.820669911399932, 9.820669911399932, 9.920669911399932, 10.020669911399931, 10.120669911399931, 10.120669911399931, 10.020669911399931, 10.120669911399931, 10.020669911399931, 10.020669911399931, 9.920669911399932, 9.820669911399932, 9.720669911399932, 9.620669911399933, 13.890319651045857, 13.890319651045857, 13.990319651045857, 14.090319651045856, 13.990319651045857, 13.890319651045857, 13.790319651045857, 13.890319651045857, 13.990319651045857, 13.990319651045857, 13.990319651045857, 14.090319651045856, 14.190319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 14.390319651045855, 14.490319651045855, 14.390319651045855, 14.290319651045856, 14.190319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.290319651045856, 14.390319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.490319651045855, 14.490319651045855, 14.490319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 14.290319651045856, 14.290319651045856, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.390319651045855, 14.490319651045855, 14.590319651045855, 14.590319651045855, 14.590319651045855, 14.490319651045855, 14.590319651045855, 14.490319651045855, 14.490319651045855, 14.590319651045855, 14.590319651045855, 14.490319651045855, 14.390319651045855, 14.390319651045855, 14.290319651045856, 10.823276449575243, 10.923276449575242, 10.923276449575242, 11.023276449575242, 11.023276449575242, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.723276449575243, 10.823276449575243, 10.923276449575242, 10.923276449575242, 11.023276449575242, 11.023276449575242, 11.023276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.123276449575242, 11.023276449575242, 11.123276449575242, 11.223276449575241, 11.123276449575242, 11.023276449575242, 11.123276449575242, 11.223276449575241, 11.123276449575242, 11.123276449575242, 11.023276449575242, 10.923276449575242, 10.923276449575242, 11.023276449575242, 10.923276449575242, 10.923276449575242, 10.923276449575242, 10.823276449575243, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.823276449575243, 10.923276449575242, 10.823276449575243, 10.723276449575243, 10.623276449575243, 10.523276449575244, 10.423276449575244, 10.323276449575244, 10.223276449575245, 10.323276449575244, 10.223276449575245, 10.223276449575245, 10.223276449575245, 10.123276449575245, 10.223276449575245, 8.6847407238031, 8.6847407238031, 8.7847407238031, 8.8847407238031, 8.9847407238031, 8.9847407238031, 8.8847407238031, 8.7847407238031, 8.7847407238031, 8.6847407238031, 8.584740723803101, 8.584740723803101, 8.484740723803101, 8.484740723803101, 8.584740723803101, 8.584740723803101, 8.584740723803101, 8.584740723803101, 8.6847407238031, 8.584740723803101, 8.6847407238031, 8.7847407238031, 8.8847407238031, 8.8847407238031, 8.8847407238031, 8.8847407238031, 8.9847407238031, 9.0847407238031, 9.0847407238031, 9.184740723803099, 9.184740723803099, 9.0847407238031, 9.184740723803099, 9.284740723803099, 9.184740723803099, 9.284740723803099, 9.384740723803098, 9.484740723803098, 9.384740723803098, 9.384740723803098, 9.284740723803099, 9.384740723803098, 9.384740723803098, 9.484740723803098, 9.484740723803098, 9.584740723803097, 9.484740723803098, 9.484740723803098, 9.584740723803097, 9.484740723803098, 9.484740723803098, 9.384740723803098, 9.384740723803098, 9.484740723803098, 9.384740723803098, 9.484740723803098, 9.584740723803097, 9.684740723803097, 9.784740723803097, 9.784740723803097, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.179990113213142, 11.279990113213142, 11.179990113213142, 11.279990113213142, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.479990113213141, 11.57999011321314, 11.57999011321314, 11.67999011321314, 11.57999011321314, 11.67999011321314, 11.57999011321314, 11.57999011321314, 11.479990113213141, 11.379990113213141, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.279990113213142, 11.379990113213141, 11.379990113213141, 11.479990113213141, 11.379990113213141, 11.379990113213141, 11.279990113213142, 11.179990113213142, 11.179990113213142, 11.179990113213142, 11.079990113213142, 11.079990113213142, 11.079990113213142, 11.179990113213142, 11.079990113213142, 10.979990113213143, 10.879990113213143, 10.879990113213143, 10.979990113213143, 10.979990113213143, 10.879990113213143, 10.879990113213143, 10.779990113213143, 10.879990113213143, 10.879990113213143, 10.779990113213143, 10.679990113213144, 10.679990113213144, 10.679990113213144, 10.679990113213144, 10.579990113213144, 10.679990113213144, 10.779990113213143, 10.679990113213144, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.23006051123538, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.13006051123538, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.330060511235379, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.23006051123538, 12.13006051123538, 12.13006051123538, 12.23006051123538, 12.330060511235379, 12.330060511235379, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.330060511235379, 12.430060511235379, 12.330060511235379, 12.23006051123538, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.430060511235379, 12.530060511235378, 12.430060511235379, 12.330060511235379, 12.23006051123538, 12.23006051123538, 12.330060511235379, 12.430060511235379, 12.430060511235379, 12.530060511235378, 12.630060511235378, 12.730060511235378, 12.830060511235377, 12.730060511235378, 12.630060511235378, 12.630060511235378, 12.530060511235378, 12.530060511235378, 12.430060511235379, 12.330060511235379]\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=[\"POST\", \"GET\"])\n",
    "def mainFunction():\n",
    "    if request.method == \"POST\":\n",
    "        powerR = request.form[\"pow\"]\n",
    "        #return redirect(url_for(\"testEnv\", p=powerR))\n",
    "        return redirect(url_for(\"test\", p=powerR))\n",
    "    else:\n",
    "        return render_template(\"RL.html\")\n",
    "\n",
    "@app.route(\"/<p>\")\n",
    "def testEnv(p):\n",
    "    env= ShowerEnv(p)\n",
    "    print(p)\n",
    "    episodes = 10\n",
    "    for episode in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "    \n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            power, angles, n_state, reward, done, info = env.step(action)\n",
    "            score += reward   \n",
    "    \n",
    "    return redirect(url_for(\"showPlots\", ang=angles))\n",
    "\n",
    "@app.route(\"/<ang>\")\n",
    "def showPlots(ang):\n",
    "    figure = Figure()\n",
    "    pl = figure.add_subplot(1,1,1)\n",
    "    xs = range(60)\n",
    "    pl.plot(xs, ang)\n",
    "    output = io.BytesIO()\n",
    "    FigureCanvas(figure).print_png(output)\n",
    "    return Response(output.getvalue(), mimetype='image/png')\n",
    "\n",
    "@app.route(\"/<p>\")\n",
    "def test(p):\n",
    "    return f\"<h1>{p*3.2}</h1>\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_simple('localhost', 9000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3727341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self, pRef):\n",
    "        \n",
    "        #Acctions up, down stay\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        #Angle array\n",
    "        self.observation_space = Box(low=np.array([5]), high=np.array([14]))\n",
    "        \n",
    "        #Set static parameters\n",
    "        self.wind_density = 1.225\n",
    "        self.radious = 2\n",
    "        \n",
    "        #Set dynamic parameters\n",
    "        self.angle = random.uniform(5.0, 14.0)\n",
    "        self.wind = 10.0\n",
    "        self.error = 100000\n",
    "        self.power_eficiency = -0.0422*self.angle + 0.5911\n",
    "        self.genPowerEuler = 0.5*self.wind_density*math.pi*pow(self.radious, 2)*pow(self.wind, 3)*self.power_eficiency\n",
    "        self.powers = []\n",
    "        self.angles = []\n",
    "        \n",
    "        #Set training time\n",
    "        self.training_length = 60\n",
    "        \n",
    "        #Saving power in variable\n",
    "        self.powerRef = pRef.astype(float)\n",
    "        \n",
    "    def step(self, action):\n",
    "        #Save the error from the previous step in a variable\n",
    "        last_error = self.error\n",
    "        \n",
    "        #Reduces training time in 1 second\n",
    "        self.training_length -= 1\n",
    "        \n",
    "        self.powers.append(self.genPowerEuler)\n",
    "        self.angles.append(self.angle)\n",
    "        \n",
    "        #Apply action\n",
    "            #0.0 - 0.1 = -0.1 (angle reduces in 0.1)\n",
    "            #0.1 - 0.1 = 0.0 (angle does not change)\n",
    "            #0.2 - 0.1 = 0.1 (angle increases in 0.1)\n",
    "        self.angle += (action/10.0) - 0.1 #AÑADIR MAS ACCIONES\n",
    "        \n",
    "        #Euler for Calculating energy\n",
    "        for t in range(1, 150):\n",
    "            self.power_eficiency = -0.0422*self.angle + 0.5911\n",
    "            self.genPowerEuler += ((0.5*self.wind_density*math.pi*pow(self.radious, 2)*pow(self.wind, 3)*self.power_eficiency)/5 - self.genPowerEuler/5)*0.5\n",
    "            #self.powers.append(self.genPowerEuler)\n",
    "        \n",
    "        #Calculates final error\n",
    "        self.error = abs(float(self.powerRef) - self.genPowerEuler)\n",
    "        #self.error.append(abs(self.genPowerEuler - self.power))\n",
    "        \n",
    "        #Calculates reward\n",
    "        if self.error < last_error:\n",
    "            reward = 1\n",
    "        elif self.error == last_error:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = -10\n",
    "        \n",
    "        #Check if the training finished\n",
    "        if self.training_length <=0 or (self.genPowerEuler >= (float(self.powerRef) - 1) and self.genPowerEuler <= (float(self.powerRef) + 1)):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            \n",
    "        #Wind disturbances\n",
    "        ###self.wind += random.uniform(-0.1,0.1)\n",
    "        \n",
    "        #placeholder for the info\n",
    "        info = {}\n",
    "        \n",
    "        #plt.plot(self.powers)\n",
    "        #plt.show()\n",
    "        \n",
    "        #Return step information\n",
    "        return self.powers, self.angles, self.angle, reward, done, info\n",
    "        #return self.angle, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        #Reset parameters\n",
    "        self.angle = random.uniform(5.0, 14.0)\n",
    "        self.wind = 10.0\n",
    "        self.power_eficiency = -0.0422*self.angle + 0.5911\n",
    "        self.genPowerEuler = 0.5*self.wind_density*math.pi*pow(self.radious, 2)*pow(self.wind, 3)*self.power_eficiency\n",
    "        self.powers = []\n",
    "        \n",
    "        #Reset training time\n",
    "        self.training_length = 60\n",
    "        \n",
    "        return self.angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594bcd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env= ShowerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eff492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cf1a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-235 Angulo:7.102907269067808\n",
      "Episode:2 Score:-169 Angulo:5.975176123869084\n",
      "Episode:3 Score:-320 Angulo:6.8335054468936764\n",
      "Episode:4 Score:-248 Angulo:11.724199876298945\n",
      "Episode:5 Score:-254 Angulo:13.761568027530656\n",
      "Episode:6 Score:-254 Angulo:11.593319253731421\n",
      "Episode:7 Score:-252 Angulo:11.837125087817867\n",
      "Episode:8 Score:-295 Angulo:10.523948546841364\n",
      "Episode:9 Score:-72 Angulo:13.699872095148656\n",
      "Episode:10 Score:-214 Angulo:9.419110872318305\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        power, angles, n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} Angulo:{}'.format(episode, score, n_state))\n",
    "    plt.plot(angles)\n",
    "    plt.show()\n",
    "    plt.plot(power)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dfbde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caf1b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, activation='relu', input_shape = states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cc7395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d2b9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29f67be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 24)                48        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9fd3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a022180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "\r",
      "    1/10000 [..............................] - ETA: 11:30 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dgarr\\anaconda3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 72s 7ms/step - reward: -0.5004\n",
      "168 episodes - episode_reward: -29.845 [-479.000, 49.000] - loss: 7.978 - mae: 14.284 - mean_q: -16.235\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -0.4518\n",
      "167 episodes - episode_reward: -27.054 [-314.000, 41.000] - loss: 11.205 - mae: 15.015 - mean_q: -12.234\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -0.1721\n",
      "167 episodes - episode_reward: -10.263 [-315.000, 45.000] - loss: 12.405 - mae: 15.662 - mean_q: -9.633\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: -0.2172\n",
      "169 episodes - episode_reward: -12.893 [-277.000, 47.000] - loss: 12.314 - mae: 15.386 - mean_q: -7.378\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -0.2603\n",
      "done, took 376.245 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18cba2ffd60>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dc38509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -44.000, steps: 60\n",
      "Episode 2: reward: 9.000, steps: 60\n",
      "Episode 3: reward: -58.000, steps: 60\n",
      "Episode 4: reward: -58.000, steps: 60\n",
      "Episode 5: reward: 13.000, steps: 60\n",
      "Episode 6: reward: 19.000, steps: 60\n",
      "Episode 7: reward: -9.000, steps: 60\n",
      "Episode 8: reward: -49.000, steps: 60\n",
      "Episode 9: reward: -58.000, steps: 60\n",
      "Episode 10: reward: 49.000, steps: 60\n",
      "Episode 11: reward: -58.000, steps: 60\n",
      "Episode 12: reward: -69.000, steps: 60\n",
      "Episode 13: reward: 47.000, steps: 60\n",
      "Episode 14: reward: 39.000, steps: 60\n",
      "Episode 15: reward: -58.000, steps: 60\n",
      "Episode 16: reward: -58.000, steps: 60\n",
      "Episode 17: reward: 49.000, steps: 60\n",
      "Episode 18: reward: -58.000, steps: 60\n",
      "Episode 19: reward: -54.000, steps: 60\n",
      "Episode 20: reward: -13.000, steps: 60\n",
      "Episode 21: reward: 49.000, steps: 60\n",
      "Episode 22: reward: -58.000, steps: 60\n",
      "Episode 23: reward: -58.000, steps: 60\n",
      "Episode 24: reward: 9.000, steps: 60\n",
      "Episode 25: reward: -51.000, steps: 60\n",
      "Episode 26: reward: -58.000, steps: 60\n",
      "Episode 27: reward: -58.000, steps: 60\n",
      "Episode 28: reward: -69.000, steps: 60\n",
      "Episode 29: reward: 11.000, steps: 60\n",
      "Episode 30: reward: -53.000, steps: 60\n",
      "Episode 31: reward: -21.000, steps: 60\n",
      "Episode 32: reward: 49.000, steps: 60\n",
      "Episode 33: reward: -58.000, steps: 60\n",
      "Episode 34: reward: 43.000, steps: 60\n",
      "Episode 35: reward: -58.000, steps: 60\n",
      "Episode 36: reward: 3.000, steps: 60\n",
      "Episode 37: reward: -58.000, steps: 60\n",
      "Episode 38: reward: 33.000, steps: 60\n",
      "Episode 39: reward: -7.000, steps: 60\n",
      "Episode 40: reward: -15.000, steps: 60\n",
      "Episode 41: reward: 47.000, steps: 60\n",
      "Episode 42: reward: -35.000, steps: 60\n",
      "Episode 43: reward: 49.000, steps: 60\n",
      "Episode 44: reward: 49.000, steps: 60\n",
      "Episode 45: reward: 49.000, steps: 60\n",
      "Episode 46: reward: -39.000, steps: 60\n",
      "Episode 47: reward: 13.000, steps: 60\n",
      "Episode 48: reward: 19.000, steps: 60\n",
      "Episode 49: reward: 35.000, steps: 60\n",
      "Episode 50: reward: 49.000, steps: 60\n",
      "Episode 51: reward: -58.000, steps: 60\n",
      "Episode 52: reward: -65.000, steps: 60\n",
      "Episode 53: reward: 17.000, steps: 60\n",
      "Episode 54: reward: -63.000, steps: 60\n",
      "Episode 55: reward: -39.000, steps: 60\n",
      "Episode 56: reward: -11.000, steps: 60\n",
      "Episode 57: reward: 37.000, steps: 60\n",
      "Episode 58: reward: -58.000, steps: 60\n",
      "Episode 59: reward: 49.000, steps: 60\n",
      "Episode 60: reward: 49.000, steps: 60\n",
      "Episode 61: reward: -48.000, steps: 60\n",
      "Episode 62: reward: 49.000, steps: 60\n",
      "Episode 63: reward: 1.000, steps: 60\n",
      "Episode 64: reward: -58.000, steps: 60\n",
      "Episode 65: reward: -43.000, steps: 60\n",
      "Episode 66: reward: 49.000, steps: 60\n",
      "Episode 67: reward: 5.000, steps: 60\n",
      "Episode 68: reward: -9.000, steps: 60\n",
      "Episode 69: reward: 37.000, steps: 60\n",
      "Episode 70: reward: -1.000, steps: 60\n",
      "Episode 71: reward: -39.000, steps: 60\n",
      "Episode 72: reward: -15.000, steps: 60\n",
      "Episode 73: reward: 25.000, steps: 60\n",
      "Episode 74: reward: -25.000, steps: 60\n",
      "Episode 75: reward: 9.000, steps: 60\n",
      "Episode 76: reward: -58.000, steps: 60\n",
      "Episode 77: reward: -58.000, steps: 60\n",
      "Episode 78: reward: -1.000, steps: 60\n",
      "Episode 79: reward: -58.000, steps: 60\n",
      "Episode 80: reward: -37.000, steps: 60\n",
      "Episode 81: reward: -39.000, steps: 60\n",
      "Episode 82: reward: 49.000, steps: 60\n",
      "Episode 83: reward: 27.000, steps: 60\n",
      "Episode 84: reward: 49.000, steps: 60\n",
      "Episode 85: reward: 7.000, steps: 60\n",
      "Episode 86: reward: 49.000, steps: 60\n",
      "Episode 87: reward: 15.000, steps: 60\n",
      "Episode 88: reward: -58.000, steps: 60\n",
      "Episode 89: reward: -15.000, steps: 60\n",
      "Episode 90: reward: -58.000, steps: 60\n",
      "Episode 91: reward: 7.000, steps: 60\n",
      "Episode 92: reward: 7.000, steps: 60\n",
      "Episode 93: reward: -33.000, steps: 60\n",
      "Episode 94: reward: 49.000, steps: 60\n",
      "Episode 95: reward: -33.000, steps: 60\n",
      "Episode 96: reward: -39.000, steps: 60\n",
      "Episode 97: reward: -58.000, steps: 60\n",
      "Episode 98: reward: -23.000, steps: 60\n",
      "Episode 99: reward: 25.000, steps: 60\n",
      "Episode 100: reward: -53.000, steps: 60\n",
      "-11.5\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68fd7a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15080/2858134921.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m     return func.predict(\n\u001b[0m\u001b[0;32m    971\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    696\u001b[0m               **kwargs):\n\u001b[0;32m    697\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m     x, _, _ = model._standardize_user_data(\n\u001b[0m\u001b[0;32m    699\u001b[0m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0;32m    700\u001b[0m     return predict_loop(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2333\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m     return self._standardize_tensors(\n\u001b[0m\u001b[0;32m   2336\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[1;31m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m       x = training_utils_v1.standardize_input_data(\n\u001b[0m\u001b[0;32m   2364\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m           \u001b[0mfeed_input_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m     data = [\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m     ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    587\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     data = [\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m     ]\n\u001b[0;32m    591\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[1;34m(x, expected_shape)\u001b[0m\n\u001b[0;32m    503\u001b[0m         'Expected an array data type but received an integer: {}'.format(x))\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m   if (x.shape is not None and len(x.shape) == 1 and\n\u001b[0m\u001b[0;32m    506\u001b[0m       (expected_shape is None or len(expected_shape) != 1)):\n\u001b[0;32m    507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} Angulo:{}'.format(episode, score, n_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f8a75d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe356f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
